{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using tf.similarity on mnist dataset\n",
    "\n",
    "This tutorial uses tf.similiarity package to show how we can use tf.similarity on the mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell if you want to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf\n",
    "from absl import app, flags\n",
    "from tensorflow.keras.layers import (Conv2D, Dense, Dropout, Flatten, Input,\n",
    "                                     MaxPooling2D, Reshape, UpSampling2D)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tempfile\n",
    "import tabulate\n",
    "from tensorflow_similarity.api.engine.decoder import SimpleDecoder\n",
    "from tensorflow_similarity.api.engine.preprocessing import Preprocessing\n",
    "from tensorflow_similarity.api.engine.simhash import SimHash\n",
    "from tensorflow_similarity.experiments.mnist.augments import ImageAugmentation, PreprocessTo2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in mnist dataste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative datapath to the downloaded mnist dataset.\n",
    "DEFAULT_MNIST_DATA_PATH = \"./mnist.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mnist_data(data_path):\n",
    "    \"\"\" Returns the mnist data.\n",
    "    \n",
    "    Opens the data file specified by the argument, read each\n",
    "    line and puts 20% of the data into the testing set.\n",
    "    \n",
    "    Args:\n",
    "        data_path: A string that points to the cached mnist\n",
    "            dataset.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple that contains three elements. The first element\n",
    "        is a tuple that contains data used for training and\n",
    "        the second element is a tuple that contains data used\n",
    "        for testing. The third element is a tuple that contains\n",
    "        the target data. All three tuples have the same\n",
    "        structure, they contains two elements. The first\n",
    "        element contains a dictionary for the specs of mnist data\n",
    "        (in 2d np array), the second element contains\n",
    "        an np array of labels of class.\n",
    "    \"\"\"\n",
    "    \n",
    "    (x_train, y_train), (x_test_raw, y_test_raw) = tf.keras.datasets.mnist.load_data(path=data_path)\n",
    "\n",
    "    x_tests = []\n",
    "    y_tests = []\n",
    "\n",
    "    x_targets = []\n",
    "    y_targets = []\n",
    "\n",
    "    seen = set()\n",
    "    for x, y in zip(x_test_raw, y_test_raw):\n",
    "        if y not in seen:\n",
    "            seen.add(y)\n",
    "            x_targets.append(x)\n",
    "            y_targets.append(y)\n",
    "        else:\n",
    "            x_tests.append(x)\n",
    "            y_tests.append(y)\n",
    "\n",
    "    return (({\n",
    "        \"example\": np.array(x_train)\n",
    "    }, np.array(y_train)), ({\n",
    "        \"example\": np.array(x_tests)\n",
    "    }, np.array(y_tests)), ({\n",
    "        \"example\": np.array(x_targets)\n",
    "    }, np.array(y_targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tower models, decoder, and preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"A simple tower model for mnist dataset.\n",
    "    \n",
    "    Returns:\n",
    "        model: A tensorflow model.\n",
    "    \"\"\"\n",
    "    \n",
    "    i = Input(shape=(28, 28, 1), name=\"example\")\n",
    "    o = Conv2D(\n",
    "        32,\n",
    "        kernel_size=(5, 5),\n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        input_shape=(28, 28, 1))(i)\n",
    "    o = Conv2D(\n",
    "        32,\n",
    "        kernel_size=(5, 5),\n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        input_shape=(28, 28, 1))(i)\n",
    "    o = MaxPooling2D(pool_size=(2, 2))(o)\n",
    "    o = Dropout(.25)(o)\n",
    "\n",
    "    o = Conv2D(64, (3, 3), padding='same', activation='relu')(o)\n",
    "    o = Conv2D(64, (3, 3), padding='same', activation='relu')(o)\n",
    "    o = MaxPooling2D(pool_size=(2, 2))(o)\n",
    "    o = Dropout(.25)(o)\n",
    "\n",
    "    o = Flatten()(o)\n",
    "    o = Dense(256, activation=\"relu\")(o)\n",
    "    o = Dropout(.25)(o)\n",
    "    o = Dense(100)(o)\n",
    "    model = Model(inputs=i, outputs=o)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDecoder(SimpleDecoder):\n",
    "    \"\"\"A Decoder class for MNIST dataset.\"\"\"\n",
    "    \n",
    "    def build_reconstruction_model(self):\n",
    "        \"\"\"A model that reconstruct MNIST features from embedding.\"\"\"\n",
    "        \n",
    "        i = self.create_embedding_input()\n",
    "\n",
    "        x = Dense(128)(i)\n",
    "        x = Reshape((4, 4, 8), name=\"reshape_input\")(x)\n",
    "        x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "        x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "        x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "        x = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = self.feature_shaped_dense(x)\n",
    "\n",
    "        m = Model(inputs=i, outputs=[x])\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(Preprocessing):\n",
    "    \"\"\"A Preprocessing class that normalize the MNIST example inputs.\"\"\"\n",
    "    \n",
    "    def preprocess(self, img):\n",
    "        \"\"\"Normalized and reshape the input images.\"\"\"\n",
    "        \n",
    "        normed = img[\"example\"] / 255.0\n",
    "        normed = normed.reshape((28, 28, 1))\n",
    "        out = {\"example\": normed}\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metrics(test_metrics):\n",
    "    unpacked_test_metrics = sorted([(i[0], i[1]) for i in six.iteritems(test_metrics)])\n",
    "    print(\"\")\n",
    "    print(\"TEST\")\n",
    "    print(tabulate.tabulate(unpacked_test_metrics, [\"Metric\", \"Value\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage 1: basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mnist_example(data, model, strategy, augment, autoencoder, epochs, prewarm_epochs):\n",
    "    \"\"\"An example usage of tf.similarity with tensorboard callback.\n",
    "\n",
    "    This basic similarity run will first unpackage training,\n",
    "    testing, and target data from the arguments and then construct a\n",
    "    simple moirai model, fit the model with training data, then\n",
    "    evaluate our model with training and testing datasets.\n",
    "\n",
    "    Args:\n",
    "        data: Sets, contains training, testing, and target datasets.\n",
    "        model: tf.Model, the tower model to fit into moirai.\n",
    "        strategy: String, specify the strategy to use for mining triplets.\n",
    "        agument: Boolean, indicates whether we want to augment our data.\n",
    "        epochs: Integer, number of epochs to fit our moirai model.\n",
    "        prewarm_epochs: Integer, number of prewarm epochs for our moirai model.\n",
    "\n",
    "    Returns:\n",
    "        metrics: Dictionary, containing metrics performed on the\n",
    "            testing dataset. The key is the name of the metric and the\n",
    "            value is the np array of the metric values.\n",
    "    \"\"\"\n",
    "        \n",
    "    (x_train, y_train), (x_test, y_test), (x_targets, y_targets) = data\n",
    "\n",
    "    aux_tasks = []\n",
    "    if autoencoder:\n",
    "        task = AutoencoderTask(\"ae\",\n",
    "                               model,\n",
    "                               MNISTDecoder(name=\"mnist_decode\"),\n",
    "                               tower_names=[\"anchor\"],\n",
    "                               field_names=[\"example\"],\n",
    "                               **kwargs)\n",
    "        aux_tasks.append(task)\n",
    "\n",
    "    aug = None\n",
    "    if augment:\n",
    "        aug = ImageAugmentation()\n",
    "\n",
    "    moirai = SimHash(\n",
    "        model,\n",
    "        auxillary_tasks=aux_tasks,\n",
    "        augmentation=aug,\n",
    "        preprocessing=Normalize(),\n",
    "        strategy=strategy,\n",
    "        optimizer=Adam(lr=.001),\n",
    "        hard_mining_directory=tempfile.mkdtemp())\n",
    "    \n",
    "    moirai.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        prewarm_epochs=prewarm_epochs if autoencoder else 0,\n",
    "        epochs=epochs)\n",
    "\n",
    "    metrics = moirai.evaluate(x_test, y_test, x_targets, y_targets)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = read_mnist_data(DEFAULT_MNIST_DATA_PATH)\n",
    "model = model_fn()\n",
    "# Strategy we want to use.\n",
    "strategy = \"stable_hard_quadruplet_loss\"\n",
    "# Whether we want to augment our data.\n",
    "augment = False\n",
    "# Whether or not we want to use auxillary autoencoder task.\n",
    "autoencoder = False\n",
    "# Number of epochs\n",
    "epochs = 5\n",
    "# Number of prewarm epochs\n",
    "prewarm_epochs = 0\n",
    "\n",
    "test_metrics = run_mnist_example(data, model, strategy, augment, autoencoder, epochs, prewarm_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_metrics(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage 2: With Visualization Callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional imports\n",
    "import datetime\n",
    "from tensorflow_similarity.api.callbacks.metrics_callbacks import MetricsCallback\n",
    "from tensorflow_similarity.api.callbacks.plugins import ConfusionMatrixCallbackPlugin\n",
    "from tensorflow_similarity.api.callbacks.plugins import ClosestItemsCallbackPlugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the below line to clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_run_with_visualization(data, model, strategy, augment, autoencoder, epochs, prewarm_epochs):\n",
    "    \"\"\"An example usage of tf.similarity with visualization callback.\n",
    "\n",
    "    This basic similarity run will first unpackage training,\n",
    "    testing, and target data from the arguments and then construct a\n",
    "    simple moirai model, fit the model with training data, then\n",
    "    evaluate our model with training and testing datasets.\n",
    "\n",
    "    Args:\n",
    "        data: Sets, contains training, testing, and target datasets.\n",
    "        model: tf.Model, the tower model to fit into moirai.\n",
    "        strategy: String, specify the strategy to use for mining triplets.\n",
    "        agument: Boolean, indicates whether we want to augment our data.\n",
    "        epochs: Integer, number of epochs to fit our moirai model.\n",
    "        prewarm_epochs: Integer, number of prewarm epochs for our moirai model.\n",
    "\n",
    "    Returns:\n",
    "        metrics: Dictionary, containing metrics performed on the\n",
    "            testing dataset. The key is the name of the metric and the\n",
    "            value is the np array of the metric values.\n",
    "    \"\"\"\n",
    "        \n",
    "    (x_train, y_train), (x_test, y_test), (x_targets, y_targets) = data\n",
    "\n",
    "    aux_tasks = []\n",
    "    if autoencoder:\n",
    "        task = AutoencoderTask(\"ae\",\n",
    "                               model,\n",
    "                               MNISTDecoder(name=\"mnist_decode\"),\n",
    "                               tower_names=[\"anchor\"],\n",
    "                               field_names=[\"example\"],\n",
    "                               **kwargs)\n",
    "        aux_tasks.append(task)\n",
    "\n",
    "    aug = None\n",
    "    if augment:\n",
    "        aug = ImageAugmentation()\n",
    "        \n",
    "    log_dir=\"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    confusion_matrix_log_dir = log_dir + \"/confusion_matrix\"\n",
    "    closest_items_log_dir = LOG_DIR + \"/closest_items\"\n",
    "    \n",
    "    confusion_matrix_plugin = ConfusionMatrixCallbackPlugin(confusion_matrix_log_dir)\n",
    "    closest_items_plugin = ClosestItemsCallbackPlugin(closest_items_log_dir)\n",
    "\n",
    "    metrics_callbacks = MetricsCallback(\n",
    "        [confusion_matrix_plugin, closest_items_plugin],\n",
    "        x_test,\n",
    "        y_test,\n",
    "        x_targets,\n",
    "        y_targets)\n",
    "    \n",
    "    callbacks = [metrics_callbacks]\n",
    "\n",
    "    moirai = SimHash(\n",
    "        model,\n",
    "        auxillary_tasks=aux_tasks,\n",
    "        augmentation=aug,\n",
    "        preprocessing=Normalize(),\n",
    "        strategy=strategy,\n",
    "        optimizer=Adam(lr=.001),\n",
    "        hard_mining_directory=tempfile.mkdtemp())\n",
    "    \n",
    "    moirai.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        prewarm_epochs=prewarm_epochs if autoencoder else 0,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks)\n",
    "\n",
    "    metrics = moirai.evaluate(x_test, y_test, x_targets, y_targets)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_mnist_data(DEFAULT_MNIST_DATA_PATH)\n",
    "model = model_fn()\n",
    "# Strategy we want to use.\n",
    "strategy = \"stable_hard_quadruplet_loss\"\n",
    "# Whether we want to augment our data.\n",
    "augment = False\n",
    "# Whether or not we want to use auxillary autoencoder task.\n",
    "autoencoder = False\n",
    "# Number of epochs\n",
    "epochs = 5\n",
    "# Number of prewarm epochs\n",
    "prewarm_epochs = 0\n",
    "\n",
    "test_metrics = similarity_run_with_visualization(data, model, strategy, augment, autoencoder, epochs, prewarm_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_metrics(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage 3: Zero/Few Shot Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_preprocess(x_train, y_train, n, train_even):\n",
    "    \"\"\" Returns training data for few/zero shot training.\n",
    "\n",
    "    Arguments:\n",
    "        x_train (np.array): An array that holds the training data, where\n",
    "            x_train[i] holds the i'th example.\n",
    "        y_train (np.array): An array that holds the training labels, where\n",
    "            y_train[j] holds the label for x_train[j].\n",
    "        n (int): Number of examples we want to train on for the filtered class.\n",
    "            If n == 0 then we have zero-shot learning (training on even only\n",
    "                if train_even is True, otherwise training on odd only).\n",
    "            If n == 1 it is one-shot learning (get 2 examples for each odd\n",
    "                number because we need 2 example for one-shot learning for\n",
    "                similiarity learning).\n",
    "            If n > 1 it is few shots learning.\n",
    "            If n >= number of training samples then this method will return\n",
    "                the original x_train and y_train provided.\n",
    "        train_even (boolean): Whether we train on evens or odds\n",
    "\n",
    "    Returns:\n",
    "        filtered_x_train (np.array): An array that holds the filtered\n",
    "            training data, where filtered_x_train[i] holds the i'th example.\n",
    "        filtered_y_train (np.array): An array that holds the filtered\n",
    "            training labels, where filtered_y_train[j] holds the label\n",
    "            for filtered_x_train[j].\n",
    "    \"\"\"\n",
    "\n",
    "    if n >= len(x_train):\n",
    "        return x_train, y_train\n",
    "\n",
    "    # in triplet/quadruplet loss learning we need 2 example to be considered\n",
    "    # one shot learning.\n",
    "    if n == 1:\n",
    "        n = 2\n",
    "\n",
    "    seen = defaultdict(int)\n",
    "    filtered_x_train = []\n",
    "    filtered_y_train = []\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        is_even = (y % 2 == 0)\n",
    "        not_seen_enough = seen[y] < n\n",
    "        if is_even == train_even or not_seen_enough:\n",
    "            seen[y] += 1\n",
    "            filtered_x_train.append(x)\n",
    "            filtered_y_train.append(y)\n",
    "\n",
    "    return filtered_x_train, filtered_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_run_with_few_shots(data, model, strategy, augment, autoencoder, epochs, prewarm_epochs):\n",
    "    \"\"\"An example usage of tf.similarity with visualization callback.\n",
    "\n",
    "    This basic similarity run will first unpackage training,\n",
    "    testing, and target data from the arguments and then construct a\n",
    "    simple moirai model, fit the model with training data, then\n",
    "    evaluate our model with training and testing datasets.\n",
    "\n",
    "    Args:\n",
    "        data: Sets, contains training, testing, and target datasets.\n",
    "        model: tf.Model, the tower model to fit into moirai.\n",
    "        strategy: String, specify the strategy to use for mining triplets.\n",
    "        agument: Boolean, indicates whether we want to augment our data.\n",
    "        epochs: Integer, number of epochs to fit our moirai model.\n",
    "        prewarm_epochs: Integer, number of prewarm epochs for our moirai model.\n",
    "\n",
    "    Returns:\n",
    "        metrics: Dictionary, containing metrics performed on the\n",
    "            testing dataset. The key is the name of the metric and the\n",
    "            value is the np array of the metric values.\n",
    "    \"\"\"\n",
    "        \n",
    "    (x_train, y_train), (x_test, y_test), (x_targets, y_targets) = data\n",
    "    \n",
    "    x_train = x_train[\"example\"]\n",
    "    \n",
    "    train_even = True\n",
    "    n = 0\n",
    "    x_train, y_train = few_shot_preprocess(x_train, y_train, n, train_even)\n",
    "    \n",
    "    x_train = {\"example\": np.array(x_train)}\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    aux_tasks = []\n",
    "    if autoencoder:\n",
    "        task = AutoencoderTask(\"ae\",\n",
    "                               model,\n",
    "                               MNISTDecoder(name=\"mnist_decode\"),\n",
    "                               tower_names=[\"anchor\"],\n",
    "                               field_names=[\"example\"],\n",
    "                               **kwargs)\n",
    "        aux_tasks.append(task)\n",
    "\n",
    "    aug = None\n",
    "    if augment:\n",
    "        aug = ImageAugmentation()\n",
    "        \n",
    "    log_dir=\"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    confusion_matrix_log_dir = log_dir + \"/confusion_matrix\"\n",
    "    \n",
    "    confusion_matrix_plugin = ConfusionMatrixCallbackPlugin(confusion_matrix_log_dir)\n",
    "    metrics_callbacks = MetricsCallback(\n",
    "        [confusion_matrix_plugin],\n",
    "        x_test,\n",
    "        y_test,\n",
    "        x_targets,\n",
    "        y_targets)\n",
    "    \n",
    "    callbacks = [metrics_callbacks]\n",
    "\n",
    "    moirai = SimHash(\n",
    "        model,\n",
    "        auxillary_tasks=aux_tasks,\n",
    "        augmentation=aug,\n",
    "        preprocessing=Normalize(),\n",
    "        strategy=strategy,\n",
    "        optimizer=Adam(lr=.001),\n",
    "        hard_mining_directory=tempfile.mkdtemp())\n",
    "    \n",
    "    moirai.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        prewarm_epochs=prewarm_epochs if autoencoder else 0,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks)\n",
    "\n",
    "    metrics = moirai.evaluate(x_test, y_test, x_targets, y_targets)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_mnist_data(DEFAULT_MNIST_DATA_PATH)\n",
    "model = model_fn()\n",
    "# Strategy we want to use.\n",
    "strategy = \"stable_hard_quadruplet_loss\"\n",
    "# Whether we want to augment our data.\n",
    "augment = False\n",
    "# Whether or not we want to use auxillary autoencoder task.\n",
    "autoencoder = False\n",
    "# Number of epochs\n",
    "epochs = 5\n",
    "# Number of prewarm epochs\n",
    "prewarm_epochs = 0\n",
    "\n",
    "test_metrics = similarity_run_with_few_shots(data, model, strategy, augment, autoencoder, epochs, prewarm_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_metrics(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
